---
layout: post
title:  "Replicating Human-level control through deep reinforcement learning Part 1: Understanding the Loss Function"
date:   2020-01-12 00:00:00 -0500
categories: machine-learning
---

This series of posts will be to write about my experience replicating an influential reinforcement learning paper published in 2015: [Human-level control through deep reinforcement learning](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf). I self-studied [CSC421 Deep Learning](http://www.cs.toronto.edu/~rgrosse/courses/csc421_2019/) which had a publically available syllabus and whose lectures were based on accessible research papers, and after completing all the [homework](https://github.com/RahmanQureshi/csc421), I wanted to do a "capstone." 

In part 1, I will briefly cover the basics of reinforcement learning, and then motivate the reason for the loss function being as it is. I will assume that the reader is at least familiar with the basics of machine learning and neural networks.

## What is Reinforcement Learning?

In reinforcement learning, an **agent** interacts with an **environment** by observing its **state** taking some **action**. The result of the agent's action is that it may obtain some **reward** from the environment and the state of the environment changes. The agent continuously observes the new state and making actions with the goal of maximizing its total reward. This problem formulation can be used to model a myriad of situations. For instance, it can be used to model the classic Atari game pacman. The agent, pacman, has 5 actions: stay still, go up, go down, go left, and go right. The environment is the board, and it is very complex; it has obstacles, ghosts which chase pacman, pellets which cause ghosts to flee and allow pacman to eat them, and points scattered all over the board that pacman must collect.


<img src="/assets/posts/reinforcement_learning_model.png" alt="Image is unavailable. Sorry." style="margin-left: auto; margin-right: auto; width: 80%; display: block;"/>

The strategy or decision-making process that the agent uses to select it's action is called a **policy**. **Reinforcement learning** is teaching a software agent the optimal policy that maximizes its reward.

## Reinforcement Learning Theory

There are many online resources for reinforcement learning. You can find many blogs covering the basics in an intuitive way, written in an almost narrative format. Then, there are [free textbooks](http://incompleteideas.net/book/the-book-2nd.html) that cover the fundamentals and advanced topics. This section is my best attempt at the former, but sprinkled with rigor insofar as it's useful to understand the paper.

### Problem Formulation

At every timestemp $$t = 0, 1, 2, ...$$ the environment is in some state $$s_t$$:
- the agent takes an action $$a_t$$
- the environment moves to some new state $$s_{t+1}$$ governed by the dynamics of the environment which is modeled by a probability distribution $$ s_{t+1}  \sim P(s_{t+1} \vert s_t, a_t) $$ 
- the agent receives reward $$ r_t $$ governed by the dynamics of the environment and modeled by a probability distribution $$ r_t \sim R(s_t, a_t) $$

As discussed, the goal of the agent is to maximize the total reward it receives: $$ r_0 + r_1 + r_2 + ...$$ In reality though, we modify the goal to maximize the total **discounted** reward: $$r_0 + \gamma r_1 + \gamma^2 r_2 + ...$$ where $$\gamma$$ is the **discount rate**. The use of the discount rate can be motivated in a lot of ways. Mathematically, it ensures the sum is not infinite and that it will converge. Economically, rewards (money) in the future are (is) worth less than rewards (money) right now. Another interpretation is that it models the agent's **uncertainty** about the future rewards, and thus, they are worth less. Here is a tangible example to motivate the use of a discount: consider a game where the agent simply needs to move around the board to pickup coins. If the rewards were not discounted, a valid solution for the agent would be to just stay still, because it doesn't matter when it moves to collect the coins.

The policy of an agent is denoted $$\pi$$. The policy can be deterministic $$a_t = \pi(s_t)$$ or probabilistic  $$a_t \sim \pi(s_t) $$.

For the purposes of this blog post, let us assume we somehow obtained a dataset of experiences $$D = [e_1, e_2, e_3, ...]$$ where $$e_t = [s_t, a_t, s_{t+1}, r_t] $$. How can we train an agent to take actions to optimize the amount of reward it collects?

### The Q-Value Function

The Q-value function $$Q^\pi(s,a)$$ represents the expected discounted reward if an agent currently in state $$s$$ takes action $$a$$ and then follows policy $$\pi$$ for every subsequent action.

$$ Q^\pi (s,a) = \mathop{\mathbb{E}} \left[ r_0 + \gamma r_1 + \gamma^2 r_2 + ... \big\vert s_0 = s, a_0 = a \right] \\

 = \mathop{\mathbb{E}} \left[ \sum_{t=0}^{\infty} \gamma^t r_t \big\vert s_0 = s, a_0 = a \right] $$

In this treatment, I will assume that the policy, state transition and rewards are all deterministic. This will allow us to simplify the equation by not having to marginalize over all possible subsequent actions or resulting states, and to get to a more **interpretable equation that elucidates and motivates the form of the loss function presented in the paper**. Therefore, we can drop the expectation, and also, let us separate the first term from the sum:

$$ Q^\pi (s,a) = r_0 (s,a) + \sum_{t=1}^{\infty} \gamma^t r_t \\

   Q^\pi (s,a) = r_0 (s,a) +  \gamma \sum_{t=0}^{\infty} \gamma^{t} r_{t+1}$$

Let the resulting state be $$s'$$, and let $$a' = \pi(s')$$. It is easy to see the second term in the equation above is just the discounted Q-value of the resulting state-action pair. That is:

$$ Q^\pi (s,a) = r (s,a) +  \gamma Q^\pi(s', a') \;\;\;\;  (1) $$

This is the recursive form of the Q-value function. The goal is to find a policy $$\pi^*$$ which maximizes the value of the Q-value function. The optimal Q-value function is:

$$ Q^*(s,a) = \sup_\pi Q^\pi(s,a) $$

And given $$Q^*$$, it is intuitively easy to see that the optimal policy $$\pi^*$$ is that which selects the action which maximizes $$Q^*$$ i.e. $$\pi^*(s) = \max_a Q^*(s, a)$$. Let me reiterate this to emphasize it's importance: **if I have the optimal Q-value function, I also have the optimal policy $$\pi^*$$**.


Following the policy of selecting the action which maximizes the Q-value function, using the form of equation $$(1)$$ the optimal Q-value function $$Q^*$$ satisfies the following equation:

 $$ Q^* (s,a) = r (s,a) +  \gamma \max_{a'} Q^*(s', a') $$


Does this look familiar?  The "target" of the squared-error loss function used in the paper is $$r + \gamma\max_{a'}Q(s', a', \theta)$$, i.e. the right-hand side of the equation, and the prediction is $$Q(s, a, \theta)$$, i.e. the left-hand side. Then, through repeated backpropagation on a large buffer of "experiences," the neural network converges to the optimal Q-value function! 

The intuition that the neural network will converge comes from the fact that the recursive definition above can be reinterpreted as a Bellman operator operating on the Q function, and the Bellman operator has a property called "contraction" which guarantees convergence to the optimal value in the context of something called **value iteration**. In value iteration, the Q function is iteratively updated according to the following rule: $$Q_{k+1}(s,a) \leftarrow r(s,a) + \gamma \max_{a'}Q_{k}(s',a') $$. I mention it for completeness, but to build your own intuition, I recommend [reading about value iteration](https://medium.com/@jonathan_hui/rl-value-learning-24f52b49c36d) and using it to solve the problem of an agent escaping from a maze.